#include <deal.II/grid/grid_in.h>
#include <deal.II/grid/grid_out.h>

#include <deal.II/base/function.h>
#include <deal.II/base/quadrature_lib.h>
#include <deal.II/dofs/dof_handler.h>
#include <deal.II/dofs/dof_tools.h>
#include <deal.II/fe/fe_q.h>
#include <deal.II/fe/fe_simplex_p.h>
#include <deal.II/fe/fe_values.h>
#include <deal.II/grid/grid_generator.h>
#include <deal.II/grid/tria.h>
#include <deal.II/numerics/matrix_tools.h>
#include <deal.II/numerics/vector_tools.h>

#include <deal.II/lac/dynamic_sparsity_pattern.h>
#include <deal.II/lac/full_matrix.h>
#include <deal.II/lac/precondition.h>
#include <deal.II/lac/solver_cg.h>
#include <deal.II/lac/solver_gmres.h>
#include <deal.II/lac/sparse_matrix.h>
#include <deal.II/lac/vector.h>

#include <deal.II/base/conditional_ostream.h>
#include <deal.II/base/mpi.h>
#include <deal.II/numerics/data_out.h>

#include <deal.II/lac/petsc_precondition.h>
#include <deal.II/lac/petsc_solver.h>
#include <deal.II/lac/petsc_sparse_matrix.h>
#include <deal.II/lac/petsc_vector.h>

#include <deal.II/dofs/dof_renumbering.h>
#include <deal.II/grid/grid_tools.h>

#include <fstream>
#include <iostream>

using namespace dealii;

template <int dim>
void print_mesh_info(const Triangulation<dim>& triangulation, const std::string& filename,
                     ConditionalOStream& pcout)
{
  pcout << "Mesh info:" << std::endl
        << " dimension: " << dim << std::endl
        << " no. of cells: " << triangulation.n_active_cells() << std::endl;

  {
    std::map<types::boundary_id, unsigned int> boundary_count;
    for (const auto& face : triangulation.active_face_iterators())
      if (face->at_boundary()) boundary_count[face->boundary_id()]++;

    pcout << " boundary indicators: ";
    for (const auto& pair : boundary_count)
    {
      pcout << pair.first << '(' << pair.second << " times) ";
    }
    pcout << std::endl;
  }

  std::ofstream out(filename);
  GridOut grid_out;
  grid_out.write_vtu(triangulation, out);
  pcout << " written to " << filename << std::endl << std::endl;
}

enum class BoundaryID
{
  wall = 4,
  inlet = 5,
  outlet = 6
};

template <int dim>
class NavierStokesProblem
{
 public:
  NavierStokesProblem()
  : mpi_comm(MPI_COMM_WORLD),
  n_mpi_proc(Utilities::MPI::n_mpi_processes(mpi_comm)),
  this_mpi_proc(Utilities::MPI::this_mpi_process(mpi_comm)),
  pcout(std::cout, (this_mpi_proc == 0)),
  fe(1),
  dof_handler(triangulation){}
  void run();

 private:
  void make_grid();
  void setup_system();
  void assemble_system();
  unsigned int solve();
  void output_results() const;

  MPI_Comm mpi_comm;
  const unsigned int n_mpi_proc;
  const unsigned int this_mpi_proc;

  ConditionalOStream pcout;

  Triangulation<dim> triangulation;
  const FE_SimplexP<dim> fe;
  DoFHandler<dim> dof_handler;

  PETScWrappers::MPI::SparseMatrix system_matrix;
  PETScWrappers::MPI::Vector solution;
  PETScWrappers::MPI::Vector system_rhs;
};

template <int dim>
class RightHandSide : public Function<dim>
{
 public:
  virtual double value(
    const Point<dim>& p, 
    const unsigned int componet = 0
  ) const override;
};

template <int dim>
class BoundaryValues : public Function<dim>
{
 public:
  virtual double value(
    const Point<dim>& p, 
    const unsigned int component = 0
  ) const override;
};

template <int dim>
double RightHandSide<dim>::value(
  const Point<dim>& p, const unsigned int ) const
{
  double return_value = 0.0;
  for (unsigned int i = 0; i < dim; ++i) 
    return_value += 4.0 * std::pow(p(i), 4.0);

  return return_value;
}

template <int dim>
double BoundaryValues<dim>::value(
  const Point<dim>& p, const unsigned int) const
{
  return p.square();
}

template <int dim>
void NavierStokesProblem<dim>::assemble_system()
{
  const QGauss<dim> quadrature_formula(fe.degree + 1);

  FEValues<dim> fe_values(
      fe, quadrature_formula,
      update_values | update_gradients | 
      update_quadrature_points | update_JxW_values
    );

  const unsigned int dofs_per_cell = fe.n_dofs_per_cell();
  const unsigned int n_q_points = quadrature_formula.size();

  FullMatrix<double> cell_matrix(dofs_per_cell, dofs_per_cell);
  Vector<double> cell_rhs(dofs_per_cell);

  std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);
  RightHandSide<dim> right_hand_side;

  for (const auto& cell : dof_handler.active_cell_iterators())
  {
    if (cell->subdomain_id() == this_mpi_proc)
    {
      fe_values.reinit(cell);

      cell_matrix = 0;
      cell_rhs = 0;

      for (const unsigned int q_index : fe_values.quadrature_point_indices())
      {
        for (const unsigned int i : fe_values.dof_indices())
          for (const unsigned int j : fe_values.dof_indices())
            cell_matrix(i, j) += (fe_values.shape_grad(i, q_index) *
                                  fe_values.shape_grad(j, q_index) * 
                                  fe_values.JxW(q_index));

        const auto& x_q = fe_values.quadrature_point(q_index);
        for (const unsigned int i : fe_values.dof_indices())
          cell_rhs(i) += (fe_values.shape_value(i, q_index)) * 
                          right_hand_side.value(x_q) *
                          fe_values.JxW(q_index);
      }
      cell->get_dof_indices(local_dof_indices);

      for (const unsigned int i : fe_values.dof_indices())
        for (const unsigned int j : fe_values.dof_indices())
          system_matrix.add(local_dof_indices[i], 
                            local_dof_indices[j], 
                            cell_matrix(i, j));

      for (const unsigned int i : fe_values.dof_indices())
        system_rhs(local_dof_indices[i]) += cell_rhs(i);
    }
  }

  system_matrix.compress(VectorOperation::add);
  system_rhs.compress(VectorOperation::add);

  std::map<types::global_dof_index, double> boundary_values;

  // // VectorTools::interpolate_boundary_values(dof_handler,
  // //                                          types::boundary_id(BoundaryID::wall),
  // //                                          Functions::ZeroFunction<dim>(),
  // //                                          boundary_values);
  VectorTools::interpolate_boundary_values(
    dof_handler, types::boundary_id(BoundaryID::wall),
    BoundaryValues<dim>(), boundary_values
  );
  // // VectorTools::interpolate_boundary_values(dof_handler,
  // //                                          types::boundary_id(BoundaryID::outlet),
  // //                                          Functions::ZeroFunction<dim>(),
  // //                                          boundary_values);

  // std::cout << boundary_values.size() << std::endl;

  MatrixTools::apply_boundary_values(
    boundary_values, system_matrix, solution, system_rhs, false
  );
}

template <int dim>
unsigned int NavierStokesProblem<dim>::solve()
{
  SolverControl solver_control(1000, 1e-6 * system_rhs.l2_norm());
  PETScWrappers::SolverCG cg(solver_control);

  PETScWrappers::PreconditionBlockJacobi preconditioner(system_matrix);
  cg.solve(system_matrix, solution, system_rhs, preconditioner);

  Vector<double> localized_solution(solution);
  solution = localized_solution;

  return solver_control.last_step();
}

template <int dim>
void NavierStokesProblem<dim>::output_results() const
{
  const Vector<double> localized_solution(solution);

  if (this_mpi_proc == 0)
  {
    std::ofstream output("solution.vtk");
    DataOut<dim> data_out;
    data_out.attach_dof_handler(dof_handler);

    data_out.add_data_vector(localized_solution, "solution");
 
    std::vector<unsigned int> partition_int(triangulation.n_active_cells());
    GridTools::get_subdomain_association(triangulation, partition_int);
 
    const Vector<double> partitioning(partition_int.begin(),
                                      partition_int.end());
    
    data_out.add_data_vector(partitioning, "partitioning");
 
    data_out.build_patches();
    data_out.write_vtk(output);                                      
  }
}

template <int dim>
void NavierStokesProblem<dim>::setup_system()
{
  GridTools::partition_triangulation(n_mpi_proc, triangulation);

  dof_handler.distribute_dofs(fe);
  DoFRenumbering::subdomain_wise(dof_handler);

  DynamicSparsityPattern dsp(dof_handler.n_dofs());
  DoFTools::make_sparsity_pattern(dof_handler, dsp);

  const std::vector<IndexSet> locally_owned_dofs_per_proc 
    = DoFTools::locally_owned_dofs_per_subdomain(dof_handler);
  const IndexSet& locally_owned_dofs 
    = locally_owned_dofs_per_proc[this_mpi_proc];

  system_matrix.reinit(locally_owned_dofs, locally_owned_dofs, dsp, mpi_comm);

  solution.reinit(locally_owned_dofs, mpi_comm);
  system_rhs.reinit(locally_owned_dofs, mpi_comm);
}

template <int dim>
void NavierStokesProblem<dim>::make_grid()
{
  GridIn<dim> gridin;
  gridin.attach_triangulation(triangulation);

  std::ifstream f("../mesh/test.msh");
  gridin.read_msh(f);

  print_mesh_info(triangulation, "grid.vtu", pcout);
}

template <int dim>
void NavierStokesProblem<dim>::run()
{
  make_grid();
  setup_system();
  assemble_system();

  const unsigned int n_iter = solve();
  pcout << "   Solver converged in " 
        << n_iter<< " iterations." 
        << std::endl;

  output_results();
}

using MyMPI = Utilities::MPI::MPI_InitFinalize;

int main(int argc, char* argv[])
{
  try
  {
    MyMPI mpi_init(argc, argv, 1);

    NavierStokesProblem<3> navier_stokes;
    navier_stokes.run();
  }
  catch (std::exception& exc)
  {
    std::cerr << "Exeption on processing: " << std::endl
              << exc.what() << std::endl
              << "Aborting .." << std::endl;
    return 1;
  }
}